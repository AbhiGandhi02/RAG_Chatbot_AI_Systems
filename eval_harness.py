"""
Bonus Challenge: Eval Harness

This script runs a predefined suite of test queries against the ClearPath RAG Chatbot
API and verifies that the LLM's response contains expected key phrases or concepts.
It uses FastAPI's TestClient to bypass Firebase Authentication for local testing.
"""

import time
import logging
import warnings
from fastapi.testclient import TestClient
from backend.main import app
from backend.auth.dependencies import get_current_user
from backend.db.models import User

from fastapi import Depends
from sqlalchemy.ext.asyncio import AsyncSession
from backend.db.database import get_db
from backend.db import crud

# Suppress overly verbose logs during tests
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("sentence_transformers").setLevel(logging.WARNING)
warnings.filterwarnings("ignore")

# Mock authenticated user to bypass Firebase for automated testing
async def override_get_current_user(db: AsyncSession = Depends(get_db)):
    user_id = "eval_harness_user"
    user_email = "eval@clearpath.test"
    
    # Ensure the user exists in the DB so Conversations don't hit Foreign Key violations
    db_user = await crud.get_user(db, user_id=user_id)
    if not db_user:
        db_user = await crud.create_user(db, user_id=user_id, email=user_email)
        
    return db_user

app.dependency_overrides[get_current_user] = override_get_current_user

# Define our test suite
# Format: (Query, [List of expected substrings/keywords in the answer])
TEST_SUITE = [
    # 1. Simple Factual Queries
    (
        "What is the price of the Pro plan?",
        ["$49", "month", "Pro"]
    ),
    (
        "Does ClearPath have a mobile app?",
        ["iOS", "Android", "app"]
    ),
    (
        "How much storage do I get on the Enterprise plan?",
        ["500GB"]
    ),
    
    # 2. Complex / Multi-part Queries
    (
        "Explain the differences between the Pro and Enterprise plans.",
        ["Pro", "Enterprise"] # Broadened since the exact wording varies (e.g., "User Limit", "Storage")
    ),
    (
        "How do I set up SSO and which plan do I need?",
        ["Enterprise", "SSO"] 
    ),
    
    # 3. Troubleshooting / Edge Cases
    (
        "My dashboard is frozen and won't load anything.",
        ["cache", "clear", "browser"]
    ),
    
    # 4. Out-of-Scope (Should trigger evaluator flags)
    (
        "What is the capital of France?",
        ["Low confidence"] # Checking for the injected evaluator warning flag
    )
]

def run_eval_harness():
    print("=" * 70)
    print("üöÄ AUTOMATED EVALUATION HARNESS")
    print("=" * 70)
    
    passed_count = 0
    total_latency = 0
    
    with TestClient(app) as client:
        for idx, (query, expected_keywords) in enumerate(TEST_SUITE, 1):
            print(f"\nTest {idx}: {query}")
            
            try:
                response = client.post("/query", json={"question": query})
                
                if response.status_code != 200:
                    print(f"  ‚ùå FAILED: API Error {response.status_code}")
                    continue
                    
                data = response.json()
                answer = data.get("answer", "")
                latency = data.get("metadata", {}).get("latency_ms", 0)
                total_latency += latency
                
                missing_keywords = [kw for kw in expected_keywords if kw.lower() not in answer.lower()]
                
                # Always show the expected and received answer
                print(f"     Expected keywords : {expected_keywords}")
                print(f"     Received answer   : {answer.strip().replace(chr(10), ' ')}")

                if not missing_keywords:
                    print(f"  ‚úÖ PASSED ({latency}ms)")
                    passed_count += 1
                else:
                    print(f"  ‚ùå FAILED ({latency}ms)")
                    print(f"     Missing keywords  : {missing_keywords}")
                    
            except Exception as e:
                print(f"  ‚ùå FAILED: Exception occurred: {e}")

    print("\n" + "=" * 70)
    print(f"üìä SUMMARY: {passed_count}/{len(TEST_SUITE)} tests passed.")
    if passed_count > 0:
        print(f"‚è±Ô∏è Average Latency: {total_latency // passed_count}ms per successful query")
    print("=" * 70)

if __name__ == "__main__":
    run_eval_harness()
